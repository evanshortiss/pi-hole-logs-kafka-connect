# Pi-hole Logs Kafka Integration

This example demonstrates how Pi-hole logs can be ingested into Red Hat
OpenShift Streams for Apache Kafka using Kafka Connect.

Of course, you can use any Kafka instance if you change the configuration to
suit, but OpenShift Streams is easier than setting up Kafka yourself!

## Requirements

* A [cloud.redhat.com](https://cloud.redhat.com) account (this is free)
* A device with Docker and Docker Compose installed

## Overview

This repository demonstrate a simple method for forwarding the dnsmasq logs
generated by Pi-hole to Apache Kafka. A summary of Pi-hole, Kafka, and Kafka
Connect are included at the end of this section for those not familiar with them.

The general idea is:

1. Create a Kafka instance and Topic to ingest Pi-hole logs.
1. Create a shared volume.
1. Start a container that runs Pi-hole:
    * Mount a custom dnsmasq config file that increases log verbosity (`log-queries=extra` param)
    * Mount the shared volume as a location for Pi-hole to write logs.
1. Start a container that runs Kafka Connect:
    * Mount a `worker.properties` configuration with Kafka connection details.
    * Mount a `connector.properties` configuration to create a `FileStreamSource`.
    * The `FileStreamSource` reads the Pi-hole logs from the shared volume and sends them to a Kafka topic.
1. Perform real-time analysis using tools of your choice that can connect to the Kafka instance.

A visualisation of this architecture:

![Architecture](/images/architecture.png)


### Pi-hole
*The Pi-holeÂ® is a DNS sinkhole that protects your devices from unwanted content, without installing any client-side software.* - Source [docs.pi-hole.net](https://docs.pi-hole.net/).

### Kafka

*Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.* - Source [kafka.apache.org](https://kafka.apache.org/)

### Kafka Connect

*Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka.* - Source [kafka.apache.org](https://kafka.apache.org/documentation/#connect)

## Usage

### Create a Kafka Instance using OpenShift Streams

Login to cloud.redhat.com and create a Kafka instance using the [Streams for Apache Kafka UI](https://cloud.redhat.com/beta/application-services/streams/kafkas).
It's free, and you don't even need to provide a credit card!

The following image illustrates a user creating a Kafka instance named
`raspberry-pi`.

![Creating a Kafka Instance](/images/cdrh-kafka-create.png)

You can obtain the bootstrap server URL once the instance has been created.

![Kafka Instance Bootstrap URL](/images/cdrh-kafka-url.png)

### Create a Service Account for your Kafka Instance

Connecting to the Kafka instance on cloud.redhat.com uses the `SASL_SSL`
protocol and `PLAIN` mechanism. This requires you to create Service Account
to obtain a username (client ID) and password (client secret).

Select the *Service Accounts* section from the side-navigation and click
*Create service account*. Enter a name, and description then click *Create*.

![Service Account Creation](/images/cdrh-sa-create.png)

This will create the Service Account and display the client ID and client
secret. Take note of these, then close the dialog.

![Service Account Details](/images/cdrh-sa-details.png)

### Create a Kafka Topic for Logs

This is the last task you'll need to perform in the OpenShift Streams UI.
Select your Kafka instance from the *Kafka Instances* list and click the
*Create topic* button.

![Topic Creation](/images/cdrh-topic-create.png)

Name the topic `pihole-dnsmasq-logs`, and make sure to set the *Partitions* option to 1. Other settings can be left at the defaults.

![Topic Name](/images/cdrh-topic-name.png)

### Running the Pi-hole and Kafka Connect

Starting the Pi-hole and forwarding the logs is made easy, thanks to the
included `docker-compose.yaml` file. The basic flow is:

1. Clone this repository
2. Use the `template.worker.properties` to create a `worker.properties` with your Bootstrap Server URL and Service Account details.
3. Run `docker-compose up`
4. Test it out!

Here are the commands you can run to try it out:

```bash
git clone $THIS_REPO_URL pihole-kafka
cd pihole-kafka

# Create a worker.properties file
cp kafka-config/template.worker.properties kafka-config/worker.properties

# Set the bootstrap URL, and SASL username and password in worker.properties
vi kafka-config/worker.properties

# If running on a Raspberry Pi you must use the "latest-arm" image tag
vi docker-compose.yaml

# Start the Pi-hole and Kafka Connect containers
docker-compose up
```

Send a few DNS queries to the Pi-hole once `docker-compose up` has finished.
Do this in another terminal using `nslookup $DOMAIN_NAME 127.0.0.1`. The
Pi-hole should return results as shown:

![nslookups in terminal](/images/nslookups.png)

### Verify Pi-hole Log Forwarding

Consume from the `pihole-dnsmasq-logs` to Topic to verify the logs have been
successfully written to it.

The [kafkacat](https://github.com/edenhill/kafkacat#kafkacat) CLI makes this a piece of cake:

```bash
# Remember to set the variables to use your bootstrap server URL
# and service account id and secret!
kafkacat -t pihole-dnsmasq-logs \
-b $BOOTSTRAP_SERVER_URL \
-X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN \
-X sasl.username=$SA_CLIENT_ID \
-X sasl.password=$SA_CLIENT_SECRET -C
```

Once Kafkacat has connected you'll be able to view the logs that were forwarded
to your Kafka Topic by Kafka Connect! The initial entries will be startup logs
from dnsmasq:

![Kafkacat initial output](/images/kafkacat-command.png)

But, if you look closely you'll be able to see your queries:

![Kafkacat initial output](/images/kafkacat-nslookups.png)

## Creating a Custom Kafka Connect Container Image

Modify the *KafkaConnect.Dockerfile* and build it:

```bash
# Change this to use your own account if you plan to push to a registry
export TAG=quay.io/evanshortiss/kafka-connect-simple:latest

docker build . -f KafkaConnect.Dockerfile -t $TAG
```

Update the *docker-compose.yaml* to use your new `$TAG` for the *image*
parameter under the *kafkaconnect* section.
